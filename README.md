# Twitter-Disaster-NLP
This repository contains the data and code for identifying disaster tweets. All the codes are written in Python 3 and stored in the Jupyter notebook. The notebook can be run directly using Anaconda Navigator or Google Colab notebook.

## Data Files
The data folder contains the following files:
- train.csv - the training set
- test.csv - the test set
- sample_submission.csv - a sample submission file in the correct format

## Analysis
The analysis folder has the Jupyter notebook that contains the code for raw data statistics and data analysis of the dataset.

## Baseline Models
The baseline folder contains the LSTM with GloVe embedding and BERT model. For the pre-trained GloVe model, it can be downloaded from [here](https://www.kaggle.com/terenceliu4444/glove6b100dtxt/download). Both jupyter notebook can be run directly after downloading the pre-trained model.

## BERT with Meta-features
Both tuned and untuned BERT with meta-features are in the root directoy. All the pre-trained model can be download directed using the code in the notebook. The notebook contains all the library imports and files.

## Run the notebook
For each Jupyter notebook, download the training and test data and save them in the same directory as the notebook. Open the notebook in a Jupyter notebook environment to run the code.
